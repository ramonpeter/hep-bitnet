{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98fb211",
   "metadata": {},
   "source": [
    "# Read the npz file and convert to txt file\n",
    "# Then extract four momentum of the leading 100 particles within the jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae612c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 100, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "data_with_labels = np.load('/home/daohan/apps/qgtagging/QG_jets_1.npz')\n",
    "data = data_with_labels['X']\n",
    "labels = data_with_labels['y']\n",
    "\n",
    "sorted_data = np.zeros_like(data)\n",
    "for i in range(data.shape[0]):\n",
    "    sorted_data[i] = data[i][np.argsort(-data[i,:,0])]\n",
    "datax = sorted_data[:,0:100,:]\n",
    "\n",
    "n_points = datax.shape[1]\n",
    "\n",
    "# Save the data to a text file\n",
    "with open('/home/daohan/apps/qgtagging/train_data1.txt', 'w') as f:\n",
    "    for i, point_cloud in enumerate(datax):\n",
    "        for point in point_cloud:\n",
    "            row_str = ' '.join([str(x) for x in point])\n",
    "            f.write(row_str)\n",
    "            f.write('\\n')\n",
    "        if (i+1) % n_points == 0:  # Add a \"#\" after every point cloud\n",
    "            f.write('#')\n",
    "        if (i+1) != data.shape[0]*n_points:\n",
    "            f.write('\\n') # Add a newline after every point except for the last one\n",
    "print(datax.shape)\n",
    "\n",
    "with open(\"/home/daohan/apps/qgtagging/train_data1.txt\", 'r') as oldf, open('/home/daohan/apps/qgtagging/train_data1y.txt', 'w') as newf:\n",
    "    lines = oldf.readlines()\n",
    "    for i, line in enumerate(lines, start=1):\n",
    "        ls = line.split()\n",
    "        if len(ls) == 4:\n",
    "            if all(float(val) != 0 for val in ls[:2]):\n",
    "                new_line = \"   \".join(ls[:4]) + \"   \\n\"\n",
    "                newf.write(new_line)\n",
    "        if i % 101 == 0 or len(ls) == 1:\n",
    "            newf.write(\"#\\n\")\n",
    "            \n",
    "label_file_path = \"/home/daohan/apps/qgtagging/train_label1.txt\"\n",
    "with open(label_file_path, 'w') as f_label:\n",
    "    for lbl in labels:\n",
    "        f_label.write(str(lbl) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94079ebc",
   "metadata": {},
   "source": [
    "# Normalize the PID information and extract (E,px,py,pz,pT,eta,phi,PID) information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833e7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "with open(\"/home/daohan/apps/qgtagging/train_data1y.txt\", 'r') as oldf, \\\n",
    "     open('/home/daohan/apps/qgtagging/train_data1z.txt', 'w') as newf:\n",
    "\n",
    "    pid_map = {\n",
    "        22: 0,    211: 0.1,  -211: 0.2,  321: 0.3,  -321: 0.4,\n",
    "        130: 0.5, 2112: 0.6, -2112: 0.7, 2212: 0.8, -2212: 0.9,\n",
    "        11: 1,    -11: 1.1,  13: 1.2,    -13: 1.3\n",
    "    }\n",
    "\n",
    "    for line in oldf:\n",
    "        ls = line.split()\n",
    "\n",
    "        if len(ls) == 4:\n",
    "            pt, eta, phi, PID = map(float, ls)\n",
    "            if phi > math.pi:\n",
    "                phi -= math.pi\n",
    "\n",
    "            px = pt * math.cos(phi)\n",
    "            py = pt * math.sin(phi)\n",
    "            pz = pt * math.sinh(eta)\n",
    "            E  = pt * math.cosh(eta)\n",
    "\n",
    "            if PID in pid_map:\n",
    "                PID = pid_map[PID]\n",
    "\n",
    "            newf.write(f\"{E} {px} {py} {pz} {pt} {eta} {phi} {PID}\\n\")\n",
    "\n",
    "        elif len(ls) == 1:  \n",
    "            newf.write(\"#\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cbe909",
   "metadata": {},
   "source": [
    "# Preparation of the particle input features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9c58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "with open(\"/home/daohan/apps/qgtagging/train_data1z.txt\",'r') as oldf:\n",
    "    lines = oldf.readlines()\n",
    "\n",
    "PTJ, ETAJ, PHIJ, EJ = [], [], [], []\n",
    "\n",
    "px_sum = py_sum = pz_sum = E_sum = 0.0\n",
    "\n",
    "for line in lines:\n",
    "    ls = line.split()\n",
    "    if len(ls) == 8:\n",
    "        px_sum += float(ls[1])\n",
    "        py_sum += float(ls[2])\n",
    "        pz_sum += float(ls[3])\n",
    "        E_sum  += float(ls[0])\n",
    "\n",
    "    elif len(ls) == 1:\n",
    "        pt_sum  = (px_sum**2 + py_sum**2)**0.5\n",
    "        eta_sum = -math.log(pt_sum / (pz_sum + (pt_sum**2 + pz_sum**2)**0.5))\n",
    "        phi_sum = math.acos(px_sum / pt_sum)\n",
    "\n",
    "        PTJ.append(pt_sum)\n",
    "        ETAJ.append(eta_sum)\n",
    "        PHIJ.append(phi_sum)\n",
    "        EJ.append(E_sum)\n",
    "\n",
    "        px_sum = py_sum = pz_sum = E_sum = 0.0\n",
    "\n",
    "decimal = 5\n",
    "i = 0 \n",
    "\n",
    "with open('/home/daohan/apps/qgtagging/train_data1a.txt','w') as newf:\n",
    "    for line in lines:\n",
    "        ls = line.split()\n",
    "        if len(ls) == 8:\n",
    "            E   = float(ls[0])\n",
    "            px  = float(ls[1])\n",
    "            py  = float(ls[2])\n",
    "            pz  = float(ls[3])\n",
    "            pt  = float(ls[4])\n",
    "            eta = float(ls[5])\n",
    "            phi = float(ls[6])\n",
    "            PID = float(ls[7])\n",
    "\n",
    "            if eta > 3.14159:\n",
    "                eta -= 3.14159\n",
    "\n",
    "            px_r  = round(px, decimal)\n",
    "            py_r  = round(py, decimal)\n",
    "            pz_r  = round(pz, decimal)\n",
    "            E_r   = round(E,  decimal)\n",
    "            pt_r  = round(pt, decimal)\n",
    "            eta_r = round(eta, decimal)\n",
    "            phi_r = round(phi, decimal)\n",
    "\n",
    "            ptc       = round(pt_r / PTJ[i], decimal)\n",
    "            Ec        = round(E_r  / EJ[i],  decimal)\n",
    "            delta_eta = round(eta_r - ETAJ[i], decimal)\n",
    "            delta_phi = round(phi_r - PHIJ[i], decimal)\n",
    "            delta_R   = round((delta_eta**2 + delta_phi**2)**0.5, decimal)\n",
    "\n",
    "            newf.write(f\"{E_r}   {px_r}   {py_r}   {pz_r}   {pt_r}   {eta_r}   {phi_r}   \"\n",
    "                       f\"{ptc}   {Ec}   {delta_eta}   {delta_phi}   {delta_R}   {PID}   \\n\")\n",
    "\n",
    "        elif len(ls) == 1:\n",
    "            i += 1\n",
    "            newf.write(\"#\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467495e3",
   "metadata": {},
   "source": [
    "# Preparation of the jet input features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7a0c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579.7002299999999\n",
      "(100000, 20)\n",
      "[ 9.16403880e+02  4.79956650e+02  1.06384820e+02  7.67024780e+02\n",
      "  4.91605651e+02  1.00000000e+00  1.00000000e+00 -1.69712004e-02\n",
      "  2.30796776e-02  6.54640336e-02  2.03250970e+02  1.07786510e+02\n",
      "  2.32954300e+01  1.70707890e+02  1.10275150e+02  2.24316279e-01\n",
      "  2.21791913e-01 -6.93120923e-03 -5.25772704e-03  1.65294968e-02]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def init_accumulator():\n",
    "    return {\n",
    "        'px': 0.0,\n",
    "        'py': 0.0,\n",
    "        'pz': 0.0,\n",
    "        'E': 0.0,\n",
    "        'detasum': 0.0,\n",
    "        'dphisum': 0.0,\n",
    "        'drsum': 0.0\n",
    "    }\n",
    "\n",
    "pids_of_interest = [0.0, 0.1, 0.2, 0.3, 0.4,\n",
    "                    0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "def init_all_accumulators():\n",
    "    accumulators = {}\n",
    "    accumulators['total'] = init_accumulator()\n",
    "    for pid_key in pids_of_interest:\n",
    "        accumulators[pid_key] = init_accumulator()\n",
    "    return accumulators\n",
    "\n",
    "acc = init_all_accumulators()\n",
    "\n",
    "PTJ = []\n",
    "EJ = []\n",
    "PXJ = []\n",
    "PYJ = []\n",
    "PZJ = []\n",
    "ETAJ = []\n",
    "PHIJ = []\n",
    "RJ   = [] \n",
    "\n",
    "EJPID  = []\n",
    "PXJPID = []\n",
    "PYJPID = []\n",
    "PZJPID = []\n",
    "PTJPID = []\n",
    "PTFPID = []\n",
    "EFPID  = []\n",
    "ETAPID = []\n",
    "PHIPID = []\n",
    "RPID   = []\n",
    "\n",
    "def finalize_event(accumulators):\n",
    "\n",
    "    total = accumulators['total']\n",
    "    pxsum = total['px']\n",
    "    pysum = total['py']\n",
    "    pzsum = total['pz']\n",
    "    Esum  = total['E']\n",
    "\n",
    "    pt_sum = math.sqrt(pxsum**2 + pysum**2) if pxsum or pysum else 0.0\n",
    "    Detasum = total['detasum'] / pt_sum if pt_sum else 0.0\n",
    "    Dphisum = total['dphisum'] / pt_sum if pt_sum else 0.0\n",
    "    Drsum   = total['drsum']   / pt_sum if pt_sum else 0.0\n",
    "\n",
    "    pid_pt_sums = {}\n",
    "    pid_E_sums  = {}\n",
    "    pid_deta    = {}\n",
    "    pid_dphi    = {}\n",
    "    pid_dr      = {}\n",
    "\n",
    "    for pid_key in pids_of_interest:\n",
    "        px_ = accumulators[pid_key]['px']\n",
    "        py_ = accumulators[pid_key]['py']\n",
    "        E_  = accumulators[pid_key]['E']\n",
    "        pt_ = math.sqrt(px_**2 + py_**2) if px_ or py_ else 0.0\n",
    "\n",
    "        # 加权平均\n",
    "        d_eta = accumulators[pid_key]['detasum'] / pt_ if pt_ else 0.0\n",
    "        d_phi = accumulators[pid_key]['dphisum'] / pt_ if pt_ else 0.0\n",
    "        d_r   = accumulators[pid_key]['drsum']   / pt_ if pt_ else 0.0\n",
    "\n",
    "        pid_pt_sums[pid_key] = pt_\n",
    "        pid_E_sums[pid_key]  = E_\n",
    "        pid_deta[pid_key]    = d_eta\n",
    "        pid_dphi[pid_key]    = d_phi\n",
    "        pid_dr[pid_key]      = d_r\n",
    "\n",
    "    PTJ_S = [pid_pt_sums[p] for p in pids_of_interest] \n",
    "    EJ_S  = [pid_E_sums[p]  for p in pids_of_interest]     \n",
    "    PTF_S = [pt / pt_sum if pt_sum else 0.0 for pt in PTJ_S]\n",
    "    EF_S  = [e  / Esum   if Esum  else 0.0 for e  in EJ_S ]\n",
    "\n",
    "    max_pt = max(PTJ_S) if PTJ_S else 0\n",
    "    idx_max_pt = PTJ_S.index(max_pt) if PTJ_S else -1\n",
    "    chosen_pid = pids_of_interest[idx_max_pt] if idx_max_pt >= 0 else None\n",
    "\n",
    "    DetasumPID = pid_deta[chosen_pid]  if chosen_pid is not None else 0.0\n",
    "    DphisumPID = pid_dphi[chosen_pid]  if chosen_pid is not None else 0.0\n",
    "    DrsumPID   = pid_dr[chosen_pid]    if chosen_pid is not None else 0.0\n",
    "\n",
    "    PTJ.append(pt_sum)\n",
    "    EJ.append(Esum)\n",
    "    PXJ.append(pxsum)\n",
    "    PYJ.append(pysum)\n",
    "    PZJ.append(pzsum)\n",
    "    ETAJ.append(Detasum)\n",
    "    PHIJ.append(Dphisum)\n",
    "    RJ.append(Drsum)\n",
    "\n",
    "    EJPID.append(max(EJ_S) if EJ_S else 0.0)\n",
    "    PXJPID.append(max(abs(accumulators[p]['px']) for p in pids_of_interest))\n",
    "    PYJPID.append(max(abs(accumulators[p]['py']) for p in pids_of_interest))\n",
    "    PZJPID.append(max(abs(accumulators[p]['pz']) for p in pids_of_interest))\n",
    "    PTJPID.append(max_pt)\n",
    "    PTFPID.append(max(PTF_S) if PTF_S else 0.0)\n",
    "    EFPID.append(max(EF_S)   if EF_S  else 0.0)\n",
    "    ETAPID.append(DetasumPID)\n",
    "    PHIPID.append(DphisumPID)\n",
    "    RPID.append(DrsumPID)\n",
    "\n",
    "    return init_all_accumulators()\n",
    "\n",
    "with open(\"/home/daohan/apps/qgtagging/train_data1a.txt\", 'r') as oldf:\n",
    "    for line in oldf:\n",
    "        ls = line.split()\n",
    "        if len(ls) == 13:\n",
    "            E   = float(ls[0])\n",
    "            px  = float(ls[1])\n",
    "            py  = float(ls[2])\n",
    "            pz  = float(ls[3])\n",
    "            pt  = float(ls[4])\n",
    "            dEta = float(ls[9])\n",
    "            dPhi = float(ls[10])\n",
    "            dR   = float(ls[11])\n",
    "            PID  = float(ls[12])\n",
    "            acc['total']['px']      += px\n",
    "            acc['total']['py']      += py\n",
    "            acc['total']['pz']      += pz\n",
    "            acc['total']['E']       += E\n",
    "            acc['total']['detasum'] += dEta * pt\n",
    "            acc['total']['dphisum'] += dPhi * pt\n",
    "            acc['total']['drsum']   += dR   * pt\n",
    "\n",
    "            if PID in acc:\n",
    "                acc[PID]['px']      += px\n",
    "                acc[PID]['py']      += py\n",
    "                acc[PID]['pz']      += pz\n",
    "                acc[PID]['E']       += E\n",
    "                acc[PID]['detasum'] += dEta * pt\n",
    "                acc[PID]['dphisum'] += dPhi * pt\n",
    "                acc[PID]['drsum']   += dR   * pt\n",
    "\n",
    "        elif len(ls) == 1:\n",
    "            acc = finalize_event(acc)\n",
    "\n",
    "EJ    = np.array(EJ)\n",
    "PXJ   = np.array(PXJ)\n",
    "PYJ   = np.array(PYJ)\n",
    "PZJ   = np.array(PZJ)\n",
    "PTJ   = np.array(PTJ)\n",
    "PTFJ  = np.ones(100000)\n",
    "EFJ   = np.ones(100000)\n",
    "\n",
    "ETAJ  = np.array(ETAJ)\n",
    "PHIJ  = np.array(PHIJ)\n",
    "RJ    = np.array(RJ)\n",
    "\n",
    "EJPID  = np.array(EJPID)\n",
    "PXJPID = np.array(PXJPID)\n",
    "PYJPID = np.array(PYJPID)\n",
    "PZJPID = np.array(PZJPID)\n",
    "PTJPID = np.array(PTJPID)\n",
    "PTFPID = np.array(PTFPID)\n",
    "EFPID  = np.array(EFPID)\n",
    "ETAPID = np.array(ETAPID)\n",
    "PHIPID = np.array(PHIPID)\n",
    "RPID   = np.array(RPID)\n",
    "\n",
    "print(EJ[5]) if len(EJ) > 5 else None\n",
    "\n",
    "final = np.column_stack((\n",
    "    EJ, PXJ, PYJ, PZJ, PTJ,\n",
    "    PTFJ, EFJ, ETAJ, PHIJ, RJ,\n",
    "    EJPID, PXJPID, PYJPID, PZJPID,\n",
    "    PTJPID, PTFPID, EFPID, ETAPID, PHIPID, RPID\n",
    "))\n",
    "print(final.shape)\n",
    "np.save('/home/daohan/apps/qgtagging/train_jet_information1.npy', final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff9521",
   "metadata": {},
   "source": [
    "# Convert to numpy format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5650a240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (100000, 100, 13)\n",
      "labels shape: (100000,)\n",
      "data shape: (100000, 100, 10)\n",
      "labels shape: (100000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_file_path = \"/home/daohan/apps/qgtagging/train_data1a.txt\"\n",
    "label_file_path = \"/home/daohan/apps/qgtagging/train_label1.txt\"\n",
    "save_path1 = \"/home/daohan/apps/qgtagging/train_data_with_labels1_original.npz\"\n",
    "save_path2 = \"/home/daohan/apps/qgtagging/train_data_with_labels1.npz\"\n",
    "\n",
    "data = []\n",
    "with open(data_file_path, 'r') as f:\n",
    "    points = []\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line == \"#\":\n",
    "            data.append(np.array(points, dtype=float))\n",
    "            points = []\n",
    "        else:\n",
    "            row = list(map(float, line.split()))\n",
    "            points.append(row)\n",
    "    if points:\n",
    "        data.append(np.array(points, dtype=float))\n",
    "\n",
    "point_counts = [len(evt) for evt in data]\n",
    "\n",
    "with open(label_file_path, 'r') as f:\n",
    "    labels = [float(line.strip()) for line in f]\n",
    "labels = np.array(labels, dtype=float)\n",
    "\n",
    "max_point_count = 100\n",
    "feature_dim = data[0].shape[1] if data else 0\n",
    "data_flat = np.zeros((len(data), max_point_count, feature_dim), dtype=float)\n",
    "\n",
    "for i, evt_array in enumerate(data):\n",
    "    count_i = len(evt_array)\n",
    "    data_flat[i, : min(count_i, max_point_count), :] = evt_array[:max_point_count]\n",
    "\n",
    "data_reduced = np.delete(data_flat, [1,2,3], axis=2)\n",
    "np.savez(save_path1, data=data_flat, labels=labels)\n",
    "np.savez(save_path2, data=data_reduced, labels=labels)\n",
    "\n",
    "\n",
    "print(\"data shape:\", data_flat.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "print(\"data shape:\", data_reduced.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a08ac",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97716aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "data = np.load('/home/daohan/apps/qgtagging/train_data_with_labels1.npz')\n",
    "data_with_labels1 = data['data'][:10000]\n",
    "labels1 = data['labels'][:10000]\n",
    "data_with_labels2 = data['data'][10000:20000]\n",
    "labels2 = data['labels'][10000:20000]\n",
    "data_with_labels3 = data['data'][20000:30000]\n",
    "labels3 = data['labels'][20000:30000]\n",
    "data_with_labels4 = data['data'][30000:40000]\n",
    "labels4 = data['labels'][30000:40000]\n",
    "data_with_labels5 = data['data'][40000:50000]\n",
    "labels5 = data['labels'][40000:50000]\n",
    "data_with_labels6 = data['data'][50000:60000]\n",
    "labels6 = data['labels'][50000:60000]\n",
    "data_with_labels7 = data['data'][60000:70000]\n",
    "labels7 = data['labels'][60000:70000]\n",
    "data_with_labels8 = data['data'][70000:80000]\n",
    "labels8 = data['labels'][70000:80000]\n",
    "data_with_labels9 = data['data'][80000:90000]\n",
    "labels9 = data['labels'][80000:90000]\n",
    "data_with_labels10 = data['data'][90000:100000]\n",
    "labels10 = data['labels'][90000:100000]\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_1.npz',data=data_with_labels1, labels=labels1)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_2.npz',data=data_with_labels2, labels=labels2)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_3.npz',data=data_with_labels3, labels=labels3)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_4.npz',data=data_with_labels4, labels=labels4)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_5.npz',data=data_with_labels5, labels=labels5)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_6.npz',data=data_with_labels6, labels=labels6)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_7.npz',data=data_with_labels7, labels=labels7)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_8.npz',data=data_with_labels8, labels=labels8)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_9.npz',data=data_with_labels9, labels=labels9)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_10.npz',data=data_with_labels10, labels=labels10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6bf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "data = np.load('/home/daohan/apps/qgtagging/train_data_with_labels1_original.npz')\n",
    "data_with_labels1 = data['data'][:10000]\n",
    "labels1 = data['labels'][:10000]\n",
    "data_with_labels2 = data['data'][10000:20000]\n",
    "labels2 = data['labels'][10000:20000]\n",
    "data_with_labels3 = data['data'][20000:30000]\n",
    "labels3 = data['labels'][20000:30000]\n",
    "data_with_labels4 = data['data'][30000:40000]\n",
    "labels4 = data['labels'][30000:40000]\n",
    "data_with_labels5 = data['data'][40000:50000]\n",
    "labels5 = data['labels'][40000:50000]\n",
    "data_with_labels6 = data['data'][50000:60000]\n",
    "labels6 = data['labels'][50000:60000]\n",
    "data_with_labels7 = data['data'][60000:70000]\n",
    "labels7 = data['labels'][60000:70000]\n",
    "data_with_labels8 = data['data'][70000:80000]\n",
    "labels8 = data['labels'][70000:80000]\n",
    "data_with_labels9 = data['data'][80000:90000]\n",
    "labels9 = data['labels'][80000:90000]\n",
    "data_with_labels10 = data['data'][90000:100000]\n",
    "labels10 = data['labels'][90000:100000]\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_1_original.npz',data=data_with_labels1, labels=labels1)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_2_original.npz',data=data_with_labels2, labels=labels2)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_3_original.npz',data=data_with_labels3, labels=labels3)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_4_original.npz',data=data_with_labels4, labels=labels4)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_5_original.npz',data=data_with_labels5, labels=labels5)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_6_original.npz',data=data_with_labels6, labels=labels6)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_7_original.npz',data=data_with_labels7, labels=labels7)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_8_original.npz',data=data_with_labels8, labels=labels8)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_9_original.npz',data=data_with_labels9, labels=labels9)\n",
    "np.savez('/home/daohan/apps/qgtagging/train_data_with_labels1_10_original.npz',data=data_with_labels10, labels=labels10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b8552",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe30753c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (10000, 100, 10)\n",
      "Final shape: (10000, 100, 5, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "infile = \"/home/daohan/apps/qgtagging/train_data_with_labels1_1.npz\"\n",
    "data = np.load(infile)\n",
    "data_with_labels = data['data']       # shape: (10000, 100, 10)\n",
    "labels = data['labels']\n",
    "\n",
    "print(\"Original shape:\", data_with_labels.shape)  # (10000, 100, 10)\n",
    "\n",
    "data_with_labels[..., 0] = np.log(data_with_labels[..., 0] + 1e-8)  # log(E)\n",
    "data_with_labels[..., 1] = np.log(data_with_labels[..., 1] + 1e-8)  # log(pt)\n",
    "\n",
    "data_with_neighbors = np.zeros((10000, 100, 21, 8), dtype=float)\n",
    "\n",
    "num_events = data_with_labels.shape[0]\n",
    "num_points = data_with_labels.shape[1]\n",
    "\n",
    "knn_neighbors = 21\n",
    "\n",
    "for i in range(num_events):\n",
    "    curr_data = data_with_labels[i]\n",
    "\n",
    "    eta_coords = curr_data[:, 2]\n",
    "    phi_coords = curr_data[:, 3]\n",
    "\n",
    "    coords = np.stack([eta_coords, phi_coords], axis=-1)\n",
    "   \n",
    "    knn = NearestNeighbors(n_neighbors=knn_neighbors)\n",
    "    knn.fit(coords)\n",
    "\n",
    "    distances, indices = knn.kneighbors(coords)\n",
    "\n",
    "    for j in range(num_points):\n",
    "        for k in range(knn_neighbors):\n",
    "            neighbor_idx = indices[j, k]\n",
    "\n",
    "            logE_neighbor  = curr_data[neighbor_idx, 0]  # log(E)\n",
    "            logPt_neighbor = curr_data[neighbor_idx, 1]  # log(pt)\n",
    "            ptc_neighbor   = curr_data[neighbor_idx, 4]\n",
    "            Ec_neighbor    = curr_data[neighbor_idx, 5]\n",
    "            PID_neighbor   = curr_data[neighbor_idx, 9]\n",
    "\n",
    "            eta_diff = curr_data[neighbor_idx, 2] - curr_data[j, 2]\n",
    "            phi_diff = curr_data[neighbor_idx, 3] - curr_data[j, 3]\n",
    "            dR = np.sqrt(eta_diff**2 + phi_diff**2)\n",
    "\n",
    "            data_with_neighbors[i, j, k, 0] = logE_neighbor\n",
    "            data_with_neighbors[i, j, k, 1] = logPt_neighbor\n",
    "            data_with_neighbors[i, j, k, 2] = ptc_neighbor\n",
    "            data_with_neighbors[i, j, k, 3] = Ec_neighbor\n",
    "            data_with_neighbors[i, j, k, 4] = eta_diff\n",
    "            data_with_neighbors[i, j, k, 5] = phi_diff\n",
    "            data_with_neighbors[i, j, k, 6] = dR\n",
    "            data_with_neighbors[i, j, k, 7] = PID_neighbor\n",
    "\n",
    "outfile = \"/home/daohan/apps/qgtagging/train_data_with_neighbors1.npz\"\n",
    "np.savez(outfile, data=data_with_neighbors, labels=labels)\n",
    "\n",
    "print(\"Final shape:\", data_with_neighbors.shape)  # (10000, 100, 21, 8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f0c11",
   "metadata": {},
   "source": [
    "# Calculate the particle interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d51d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded shape: (10000, 100, 13)\n",
      "Using N = 100 events, each with 100 particles.\n",
      "Final shape: (100, 100, 100, 6)\n",
      "Saved shape (100, 100, 100, 6) to /home/daohan/apps/qgtagging/train_interact1.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96731/1791711266.py:36: RuntimeWarning: overflow encountered in square\n",
      "  mass_raw = (E_sum**2 - px_sum**2 - py_sum**2 - pz_sum**2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) 加载数据 (10000, 100, 13)\n",
    "in_path  = '/home/daohan/apps/qgtagging/train_data_with_labels1_1_original.npz'\n",
    "out_path = '/home/daohan/apps/qgtagging/train_interact1.npz'\n",
    "\n",
    "data_dict = np.load(in_path)\n",
    "data_all  = data_dict['data']  # 形状: (10000, 100, 13)\n",
    "print(\"Loaded shape:\", data_all.shape)\n",
    "\n",
    "# 只取前 10000 个事件 (实际上正好是 10000)\n",
    "point_cloud = data_all[:10000]  \n",
    "N, n_points = point_cloud.shape[0], point_cloud.shape[1]  # (10000, 100)\n",
    "print(\"Using N =\", N, \"events, each with\", n_points, \"particles.\")\n",
    "\n",
    "# 2) 基于列 5 (eta) 和列 6 (phi) 计算两两差分 => deltaR\n",
    "eta_i = point_cloud[:, :, 5]  # eta\n",
    "phi_i = point_cloud[:, :, 6]  # phi\n",
    "\n",
    "delta_eta = eta_i[:, :, np.newaxis] - eta_i[:, np.newaxis, :]\n",
    "delta_phi = phi_i[:, :, np.newaxis] - phi_i[:, np.newaxis, :]\n",
    "deltaR = np.sqrt(delta_eta**2 + delta_phi**2)  # (10000, 100, 100)\n",
    "\n",
    "# 3) 计算 pairwise mass (log| ...)\n",
    "#    基于列 0,1,2,3 => (E, px, py, pz)\n",
    "E_col  = np.exp(point_cloud[:, :, 0])  # e^E\n",
    "px_col = np.exp(point_cloud[:, :, 1])  # e^px\n",
    "py_col = np.exp(point_cloud[:, :, 2])  # e^py\n",
    "pz_col = np.exp(point_cloud[:, :, 3])  # e^pz\n",
    "\n",
    "E_sum  = E_col[:, :, np.newaxis]  + E_col[:,  np.newaxis, :]\n",
    "px_sum = px_col[:, :, np.newaxis] + px_col[:, np.newaxis, :]\n",
    "py_sum = py_col[:, :, np.newaxis] + py_col[:, np.newaxis, :]\n",
    "pz_sum = pz_col[:, :, np.newaxis] + pz_col[:, np.newaxis, :]\n",
    "\n",
    "mass_raw = (E_sum**2 - px_sum**2 - py_sum**2 - pz_sum**2)\n",
    "mass = np.log(np.abs(mass_raw) + 1e-12)\n",
    "\n",
    "# 4) 计算 kT = log| deltaR * exp(min(pt_i, pt_j)) |\n",
    "pt_vals = point_cloud[:, :, 4]  # 第4列: pt (实际存的可能是log(pt) 或类似?)\n",
    "pt_min = np.exp(\n",
    "    np.minimum(pt_vals[:, :, np.newaxis],\n",
    "               pt_vals[:, np.newaxis, :])\n",
    ")\n",
    "kt_raw = deltaR * pt_min\n",
    "kt     = np.log(np.abs(kt_raw) + 1e-12)\n",
    "\n",
    "# 5) 计算 z = exp(min(pt_i, pt_j)) / ( exp(pt_i) + exp(pt_j) )\n",
    "pt_i = np.exp(pt_vals)[:, :, np.newaxis]\n",
    "pt_j = np.exp(pt_vals)[:, np.newaxis, :]\n",
    "z = pt_min / (pt_i + pt_j + 1e-12)\n",
    "\n",
    "# 6) 计算 ptd = log| exp(pt_i) - exp(pt_j) |\n",
    "pt_diff = pt_i - pt_j\n",
    "ptd     = np.log(np.abs(pt_diff) + 1e-12)\n",
    "\n",
    "# 7) 合并以上 5 个特征 => (N, 100, 100, 5)\n",
    "feat_stack = np.stack([deltaR, mass, kt, z, ptd], axis=-1)\n",
    "\n",
    "# 8) 增加一个空维 (for PID mask) => (N, 100, 100, 6)\n",
    "new_feature = np.concatenate(\n",
    "    [feat_stack, np.zeros((N, n_points, n_points, 1), dtype=feat_stack.dtype)],\n",
    "    axis=-1\n",
    ")\n",
    "\n",
    "# 9) 使用 第12列 PID，相等时在最后一维存放mask\n",
    "PID_col = point_cloud[:, :, 12]  # shape: (N,100)\n",
    "for i in range(n_points):      \n",
    "    for j in range(n_points):  \n",
    "        mask = (PID_col[:, i] == PID_col[:, j])  # (N,)\n",
    "        new_feature[:, i, j, 5] = mask.astype(feat_stack.dtype)\n",
    "\n",
    "# 10) 对角线置为1\n",
    "for i in range(n_points):\n",
    "    new_feature[:, i, i, :] = 1\n",
    "\n",
    "# 检查 NaN\n",
    "has_nan = np.isnan(new_feature).any()\n",
    "if has_nan:\n",
    "    print(\"Warning: new_feature contains NaN values\")\n",
    "\n",
    "print(\"Final shape:\", new_feature.shape)\n",
    "\n",
    "# 11) 保存为 npz 文件\n",
    "np.savez(out_path, data=new_feature)\n",
    "print(f\"Saved shape {new_feature.shape} to {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2914b57",
   "metadata": {},
   "source": [
    "# Calculate the jet interaction matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592c5477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 11, 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.load('/home/daohan/apps/qgtagging/train_jet_information1.npy')\n",
    "b = np.zeros((100000,11,11))\n",
    "for i in range(11):\n",
    "    b[:,i,i] = 1\n",
    "for i in range(7,10):\n",
    "    for j in range(0,5):\n",
    "        b[:,i,j] = 0\n",
    "for i in range(0,5):\n",
    "    for j in range(7,10):\n",
    "        b[:,i,j] = 0\n",
    "#first column        \n",
    "b[:,0,1] = a[:,1]/a[:,0]\n",
    "b[:,0,2] = a[:,2]/a[:,0]\n",
    "b[:,0,3] = a[:,3]/a[:,0]\n",
    "b[:,0,4] = a[:,4]/a[:,0]\n",
    "b[:,0,6] = 1\n",
    "b[:,0,10] = np.log(a[:,10])\n",
    "#second column\n",
    "b[:,1,0] = a[:,1]/a[:,0]\n",
    "b[:,1,4] = a[:,1]/a[:,4]\n",
    "b[:,1,10] = np.log(a[:,11])\n",
    "#third column\n",
    "b[:,2,0] = a[:,2]/a[:,0]\n",
    "b[:,2,4] = a[:,2]/a[:,4]\n",
    "b[:,2,10] = np.log(a[:,12])\n",
    "#fourth column\n",
    "b[:,3,0] = a[:,3]/a[:,0]\n",
    "b[:,3,10] = np.log(a[:,13])\n",
    "#fifth column\n",
    "b[:,4,0] = a[:,4]/a[:,0]\n",
    "b[:,4,1] = a[:,1]/a[:,4]\n",
    "b[:,4,2] = a[:,2]/a[:,4]\n",
    "b[:,4,5] = 1\n",
    "b[:,4,10] = np.log(a[:,14])\n",
    "#sixth column\n",
    "b[:,5,4] = 1\n",
    "b[:,5,10] = a[:,15]\n",
    "#seventh column\n",
    "b[:,6,0] = 1\n",
    "b[:,6,10] = a[:,16]\n",
    "\n",
    "\n",
    "#eighth column\n",
    "b[:,7,9] = a[:,7]/a[:,9]\n",
    "b[:,7,10] = a[:,17]\n",
    "#nineth column\n",
    "b[:,8,9] = a[:,8]/a[:,9]\n",
    "b[:,8,10] = a[:,18]\n",
    "#tenth column\n",
    "b[:,9,7] = a[:,7]/a[:,9]\n",
    "b[:,9,8] = a[:,8]/a[:,9]\n",
    "b[:,9,10] = a[:,19]\n",
    "#eleventh column\n",
    "b[:,10,0] = np.log(a[:,10])\n",
    "b[:,10,1] = np.log(a[:,11])\n",
    "b[:,10,2] = np.log(a[:,12])\n",
    "b[:,10,3] = np.log(a[:,13])\n",
    "b[:,10,4] = np.log(a[:,14])\n",
    "b[:,10,5] = a[:,15]\n",
    "b[:,10,6] = a[:,16]\n",
    "b[:,10,7] = a[:,17]\n",
    "b[:,10,8] = a[:,18]\n",
    "b[:,10,9] = a[:,19]\n",
    "\n",
    "\n",
    "\n",
    "print(b.shape)\n",
    "np.save('/home/daohan/apps/qgtagging/train_interact_1.npy', b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b614e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
